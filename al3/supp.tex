\documentclass[12pt,a4paper]{article}
% Francais UTF8
\usepackage[french]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{url}
\usepackage{graphicx}
\usepackage[small]{caption}
% Maths de l'AMS. Dispo partout
\usepackage{amsmath, amssymb}
% \usepackage{fullpage}
\usepackage[margin=2.1cm]{geometry}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}

\newtheorem{theorem}{Théorème}
\newtheorem{definition}{Définition}

\providecommand{\abs}[1]{\lvert#1\rvert}
\providecommand{\norm}[1]{\lVert#1\rVert}

\newcounter{numeroexo}
\newcommand{\exercice}{\medskip\par\noindent\stepcounter{numeroexo}
  \hspace{-.25cm}{\textbf{Exercice \arabic{numeroexo}}}\quad}

\begin{document}
\noindent Université Paris Dauphine\\
Algèbre linéaire 3\\
2010-2011
\begin{center}
  \LARGE Algèbre linéaire 3 : exercices supplémentaires
\end{center}
\vspace{1cm}
Exercices totalement optionnels. Commencez par
chercher sans lire les indices donnés en bas de page. [$\star$] : il y a
juste à écrire en étant rigoureux, [$\star\star$], il faut un peu réfléchir, [$\star\star\star$], il faut
beaucoup réfléchir.
\section{Espaces vectoriels}
\exercice [$\star\star$] Montrer que toute matrice de rang $1$ s'écrit sous la forme
$M = x y^T$ avec $(x,y)\in\mathbb{R}^n\times\mathbb{R}^n$.

\exercice [$\star$] (Opérations sur des espaces vectoriels) Soit $E$ un espace
vectoriel, $S_1$ et $S_2$ deux sous-espaces. Dites si les espaces
suivants sont des espaces vectoriels. Dans le cas où $E$ est de
dimension finie, donner leur dimension, et un procédé de construction
d'une base à partir de celles de $S_1$ et $S_2$.
\begin{align*}
  E_1 &= S_1 \cap S_2\\
  E_2 &= S_1 \cup S_2\\
  E_3 &= S_1 + S_2 = \{x + y, x \in S_1, y \in S_2\}\\
  E_4 &= S_1 \times S_2 = \{(x, y), x \in S_1, y \in S_2\}\\
\end{align*}

Pour $E_4$, qui n'est pas un sous-ensemble de $E$, définir les
opérations d'addition et de multiplication par un scalaire.

\exercice [$\star\star\star$] (Interpolation, matrice de Vandermonde) On considère le problème d'interpolation
polynomiale : étant donné des réels $(x_i)_{1 \leq i \leq n}$ et
$(y_i)_{1 \leq i \leq n}$, on cherche un polynome $P$ de degré $n-1$
qui interpole le jeu de valeurs, c'est-à-dire tel que
\begin{align*}
  P(x_i) = y_i, 1 \leq i \leq n
\end{align*}
\begin{itemize}
\item Réécrire ce problème sous la forme d'une équation matricielle $V
  A = Y$, o\`u $A$ est le vecteur des coefficients du polynome $P$ :
  $P(X) = a_0 + a_1 X + \dots + a_{n-1} X^{n-1}$ et $Y$ le vecteur des
  $y_i$.  La matrice $V$ est appelée matrice de Vandermonde associée
  aux points $(x_i)$.
\item Démontrer que si les $x_i$ sont tous distincts, la matrice $V$
  est inversible, et ainsi prouver l'existence et l'unicité du
  problème d'interpolation polynomiale. \footnote{Indice : on pourra
    montrer que si $V A = 0$, alors $P$ est nul.}
\item Donner l'expression explicite du polynome correspondant à la
  $j$-ième colonne de l'inverse de $V$ (c'est-à-dire dont les
  coefficients sont égaux à la $j$-ième colonne de $V^{-1}$)
  \footnote{Indice: on pourra montrer qu'il correspond au problème
    d'interpolation $P(x_i) = 1$ si $i = j$, 0 sinon, et résoudre ce
    problème explicitement en factorisant $P$}. Ces polynômes sont
  appelés polynômes de Lagrange. Ils s'interprètent naturellement
  comme étant la base antéduale de la base de l'espace dual $l_i : P
  \to P(x_i)$ (la base antéduale d'une base de l'espace dual $l_i$ est
  une base $e_i$ dont la base duale est $l_i$, c'est-à-dire vérifiant
  $l_i(e_j) = \delta_{i,j}$)
\item Que se passe-t-il si on considère des polynomes
  de degré supérieur ou inférieur à $n-1$ ?
\item On cherche à calculer le déterminant de la matrice de
  Vandermonde $D_n(x_1, x_2, \dots, x_n)$. On admet que le déterminant
  est une fonction polynomiale de degré $n-1$ des $x_i$ (le vérifier
  sur $n = 2$ ou $n = 3$; on peut le démontrer avec une bonne
  définition du déterminant). Factoriser ce polynome, et en déduire
  $D_n(x_1, x_2, \dots, x_n)$ à une constante $k_n$ près. Calculer
  $k_n$ pour $n = 1, 2, 3$ (prendre des $x_i$ où le calcul est
  simple), et conjecturer la valeur de $k_n$. On peut démontrer cette
  conjecture avec des notions plus avancées sur les déterminants.
\end{itemize}

\exercice [$\star$] (Équivalence de normes) Prouver les inégalités suivantes sur
$\R^N$. À chaque fois, donner un cas d'égalité, ce qui prouve que les
inégalités sont optimales (on ne peut pas remplacer les constantes par
des constantes plus faibles)\footnote{Indice : pour la première,
  utiliser Cauchy-Schwartz}
\begin{align*}
  \norm{x}_1 &\leq \sqrt N \norm{x}_2\\
  \norm{x}_2 &\leq \sqrt N \norm{x}_\infty\\
  \norm{x}_\infty &\leq N \norm{x}_1
\end{align*}

Ceci prouve que ces trois normes sont équivalentes (c'est-à-dire
définissent la même topologie : même notions de continuité, de
limites, etc.). Ce sont des cas particuliers du théorème fondamental :
toutes les normes sont équivalentes en dimension finie.

\exercice [$\star\star$] (Norme induite) Soit $f$ une application linéaire sur
$(E,\norm{.})$ de dimension finie. On définit la \emph{norme induite}
par $\norm{.}$ :
\begin{align*}
  N(f) = \sup \left\{\frac{\norm{f(x)}}{\norm{x}}\; \Big|\; x\in E
    \setminus\{0\}\right\} =  \sup
  \left\{\norm{f(x)} \;\big|\; x\in
    E,\;\norm{x} = 1\right\}.
\end{align*}

Montrer que $N$ est bien définie, puis que c'est une norme sur
l'espace $\text{End}(E)$ des endomorphismes de $E$.

\exercice [$\star\star$] (Normes de matrices) Soit une norme $\norm{.}$ sur
$\R^n$. La norme induite par $\norm{.}$ (cf exercice précédent)
définit une norme sur l'espace des matrices $\mathcal{M}_n(\mathbb{R})$
\begin{itemize}
\item Montrer que la norme induite par $\norm{.}_1$ s'exprime comme
  \[N(A)=\max\limits_{1 \leq j \leq n} \sum _{i=1} ^n |a_{ij}|\quad\quad\text{(somme sur les lignes)}\]
\item Montrer que la norme induite par $\norm{.}_\infty$ s'exprime comme
  \[N(A)=\max\limits_{1 \leq i \leq n} \sum _{j=1} ^n |a_{ij}|\quad\quad\text{(somme sur les colonnes)}\]
\item (Nécessite le théorème spectral, 4.4 du cours) Montrer que, pour
  le cas particulier d'une matrice symétrique, la norme induite par
  $\norm{.}_2$ est $|\lambda|_\text{max}$, la plus grande valeur
  propre en valeur absolue. [Difficile] Généraliser à une matrice non
  symétrique\footnote{Indice : écrire les conditions d'optimalité pour
    obtenir que le $x$ qui maximise $||Ax||/||x||$ est vecteur propre
    de $A^T A$ (on peut utiliser la première forme de N(A) ou la
    deuxième via la méthode des multiplicateurs de
    Lagrange). Conclure.}.
\end{itemize}

\section{Produit scalaire}

\exercice [$\star$] (Polynomes orthogonaux) On considère l'espace $E_n$ des
polynomes de degré $n$. On considère la fonction
\begin{align*}
  \langle P, Q\rangle &= \int_0^1 P Q.
\end{align*}

Montrer que c'est un produit scalaire. Donner une base orthonormée de
$E_0, E_1, E_2$\footnote{Indice : on pourra orthonormaliser la base
  canonique de l'espace des polynomes grâce au procédé de
  Gram-Schmidt.}. La base ainsi obtenue forme les polynomes de
Legendre. Il existe toute une ménagerie de polynomes orthogonaux pour
différents produits
scalaires\footnote{\url{http://fr.wikipedia.org/wiki/Polynomes_orthogonaux#Tableau_des_polynomes_orthogonaux_classiques}},
avec des applications en physique, théorie de l'approximation, etc.


\exercice [$\star$] (Polynomes trigonométriques) On considère l'espace vectoriel
(le vérifier) $E$ des fonctions continues, périodiques de période
$2\pi$. Montrer que
\begin{align*}
  \langle f,g \rangle &= \int_0^{2\pi} f g
\end{align*}
est un produit scalaire sur $E$. On considère la famille infinie
$\{\cos(nx), \sin(nx), n \in \Z\}$ d'éléments de $E$. Montrer que
cette famille est orthogonale. Comment modifier cette famille pour
l'orthonormaliser ? Comment modifier le produit scalaire sans toucher
à la famille pour l'orthonormaliser ?

On peut montrer qu'en un certain sens, la famille $\{\cos(nx),
\sin(nx), n \in \Z)$ est une ``base de dimension infinie'' de $E$. En
d'autres termes, pour tout $f$ dans $E$, on a
\begin{equation*}
  f(x) = \sum_{n \in \Z} a_n \cos(nx) + b_n \sin(nx).
\end{equation*}
Comme la base est orthogonale, les coefficients $a_n$ et $b_n$
s'obtiennent comme projection orthogonale de $f$ sur la base. On a
donc
\begin{align*}
  a_n &= \frac{\langle f, \cos(nx) \rangle}{\langle \cos(nx), \cos(nx)
    \rangle}\\
  &= \frac 1 {2\pi} \int_0^{2\pi} f(x) \cos(nx) dx
\end{align*}
et de même
\begin{align*}
  b_n &= \frac 1 {2\pi} \int_0^{2\pi} f(x) \sin(nx) dx
\end{align*}

Cette décomposition de $f$ en somme de signaux élémentaires (appelés
harmoniques) a d'innombrables applications, en mathématiques pures
(équations aux dérivées partielles, théorie des nombres ...) et
appliquées (JPEG, MP3 ... à chaque fois le principe est de garder un
nombre fini de coefficients $a_n$ et $b_n$ pour reconstituer le signal
à partir d'un nombre réduit de variables) ainsi qu'en physique
(électronique, traitement du signal ... par exemple, les égaliseurs
des chaînes hifi).


\exercice [$\star\star\star$] (Alternative de Fredholm) On s'intéresse à l'équation
linéaire $A x = b$ sur $\R^n$. On sait que quand $A$ est inversible,
il existe une unique solution. Quand $A$ est non inversible, il y a
soit une infinité soit aucune solution. On va préciser cette
alternative.
\begin{itemize}
\item Montrer que si il existe $x$ tel que $A x = b$, alors pour tout
  $y \in \ker(A^T)$, $y^T b = 0$.
\item Montrer que $\dim(\ker(A)) = \dim(\ker(A^T))$
\item En déduire que $\text{Im}(A)$ est un supplémentaire orthogonal
  de $\ker(A^T)$.
\item Conclure l'alternative de Fredholm : soit $b \in
  \ker(A^T)^\perp$, auquel cas $A x = b$ admet une infinité de
  solutions, soit $b \not \in \ker(A^T)^\perp$, auquel cas $A x = b$
  n'admet pas de solutions.
\item Vérifier sur un exemple simple que cette formulation formalise
  la notion intuitive : pour qu'un système avec des équations qui se
  déduisent les unes des autres ait une solution, il faut que les
  équations soient compatibles entre elles\footnote{Indice : $y \in
    \ker(A^T)$ est le vecteur des combinaisons linéaires
    d'équations}.
\end{itemize}
L'alternative de Fredholm donne les équations permettant de tester si
$A x = b$ a une solution. Elle admet des généralisations à certains
cas en dimension infinie, qui sont par exemple utilisées pour des
questions d'existence d'équations différentielles.

\section{Projecteurs}
\exercice [$\star$] Montrer que la matrice $M = x x^T$ représente la projection
orthogonale sur le sous-espace engendré par $x$. En s'inspirant de
cette forme, donner l'expression de la matrice de la projection
orthogonale sur un sous-espace.

\exercice [$\star\star\star$] (Moindres carrés) Reformuler les résultats du cours en
utilisant le langage matriciel, sous la forme: trouver $x \in \R^n$
qui minimise
\begin{align*}
  F(x) = \norm{A x - b}_2^2,
\end{align*}
o\`u $b \in \R^m$ et $A \in M_{n,m}(\R)$ sont fixés. $n$ est le nombre
de paramètres, $m$ le nombre d'observations, $m \gg n$.

On rappelle les conditions nécessaires pour que $x$ soit le minimum de
$F$:
\begin{align*}
  \forall y \in \R^n, \frac{d}{d \varepsilon} F(x+\varepsilon y) = 0
\end{align*}
(ou, autrement dit, $\nabla F(x) = 0$). En déduire que, pour le
minimum, on a
\begin{align*}
  A^T A x = A^T b
\end{align*}

Ces équations (les ``équations normales'') expriment les conditions
d'orthogonalité vues en cours (le vérifier sur le cas bien connu de la
droite des moindres carrés).
\end{document}
